<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>


        h1, h2, h3 {
            font-weight: normal;
        }


        .footnote {
            position: absolute;
            bottom: 3em;
        }

        .diagram img {
            width: 100%;
        }

        rect.reads {
          fill: red;
          stroke: blue;
          height: 100;
        }

        .mtable table {
          margin-left: auto;
          margin-right: auto;
        }
        .mtable th {
          border-bottom:1px solid #333333;
          /*padding-right: 10px;*/
        }

        .mtable td {
          padding-right: 30px;
        }

        .mcode {
          text-align: left;
          margin-left: auto;
          margin-right: auto;
          display: inline-block;
        }
    </style>
  </head>
  <body>
    <textarea id="source">
name: Title
class: center, middle

# A billion rows pet-project on a desktop hardware
#### Alexey Papulovskiy


???
https://remarkjs.com

---
class: center
# PTR

### "Every Internet-reachable host should have a name"

8.8.4.4 → 4.4.8.8.in-addr.arpa → google-public-dns-b.google.com.


???

RFC 1912

PTR - pointer to canonical name, used for determination of a domain name associated with an IP address.

"For every IP address, there should be a matching PTR record in the in-addr.arpa domain."

"Reverse" IP ordering to not to overload DNS-servers

---
class: center
# IP v4

## 2<sup>32</sup>

4 294 967 296 possible IP addresses

3 702 258 432 non-reserved IP addresses

1 294 913 005 addresses have pointer records

???

There are reserved blocks of IP addresses

---
class: center

# reverse.report

### Database of pointer records for IPv4 addresses

1 294 913 005 records

100 GB of data

130 GB of indexes

&lt; 20 select requests per second

~ 160 000 000 updates per day

~ 2000 (adjusted) updates per second


???

The goal was to collect data and make it available for searching


---
class: center

# Why

.mtable[
| IP          | PTR                           |
|:------------|:------------------------------|
| 5.57.16.136 | bluejeans.booking.com         |
| 5.57.16.202 | derp.booking.com              |
| 5.57.16.203 | nagios-lhr4.api.booking.com   |
| 5.57.16.22  | flog.booking.com              |
| 5.57.16.29  | paste.booking.com             |
| 5.57.16.46  | canteen.booking.com           |
| 5.57.17.136 | bluejeans.booking.com         |
| 5.57.17.14  | controlrooms.booking.com      |
| 5.57.17.15  | hadoop-dashboard.booking.com  |
| 5.57.17.203 | nagios-ams4.api.booking.com   |
| 5.57.20.195 | pcln-xml-test.booking.com     |
| 5.57.20.196 | ctrip-xml-test.booking.com    |
]

???
Security research - many companies reveal infrastructure via reverse DNS

---
class: center
# Why

.mtable[
| IP             | PTR             |
|:---------------|:----------------|
| 173.1.57.105   | mysql1.uber.com |
| 173.1.57.106   | mysql2.uber.com |
| 173.1.57.107   | mongo1.uber.com |
| 173.1.57.108   | mongo2.uber.com |
| 173.204.236.25 | mongo3.uber.com |
| 173.204.236.26 | mysql3.uber.com |
]

???

---
class: center
# Why

.mtable[
| IP              | PTR                |
|:----------------|:-------------------|
| 5.9.96.242      | aspmx.l.google.com |
| 8.19.180.13     | aspmx.l.google.com |
| 23.246.203.2    | aspmx.l.google.com |
| 70.168.243.204  | aspmx.l.google.com |
| 80.77.52.162    | aspmx.l.google.com |
| 89.19.28.24     | aspmx.l.google.com |
| 96.27.130.206   | aspmx.l.google.com |
| 108.89.77.153   | aspmx.l.google.com |
| 110.143.19.194  | aspmx.l.google.com |
| 138.201.19.34   | aspmx.l.google.com |
| 146.255.185.92  | aspmx.l.google.com |
| 176.9.108.76    | aspmx.l.google.com |
| 181.170.194.209 | aspmx.l.google.com |
| 190.210.81.241  | aspmx.l.google.com |
| 200.42.6.196    | aspmx.l.google.com |
]

???
Impersonation
---
class: center
# Desktop hardware


4 cores (8 threads) CPU<!--  @4GHz with 8MB L3 cache -->

64GB non-ECC DDR4 RAM

512GB NVMe SSD

<!-- 1TB SATA SSD -->

4TB Spinning disk

500mbps internet connection

???
TODO: refine
---

title: SSD matters

<!--
  http://jsfiddle.net/gh/get/jquery/1.9.1/highslide-software/highcharts.com/tree/master/samples/highcharts/demo/column-parsed/
-->

.diagram[![SSD IOPS comparison](assets/ssd_chart_log.svg)]

???
Storage revolution

Measured with fio tool (75% reads, 25% writes) on 4GB file.

It's hard to compare environments, so the measurement done just to see the trend, not for absolute values.

Logarithmic scale just to make spinning disk visible

---

title: SSD matters

.diagram[![SSD IOPS comparison](assets/ssd_chart_linear.svg)]

---
class: center

# Scenario #1

.mcode[
```
# SELECT ip, name FROM rev WHERE ip << '8.8.8.0/24';


    ip    |             name
----------+--------------------------------
 8.8.8.8  | google-public-dns-a.google.com
 8.8.8.14 | localhost
(2 rows)

```]

???
Inet has range operator

---
class: center

# Scenario #2

.mcode[
```
# SELECT ip, name FROM rev WHERE name = 'postgresql.org'
  OR name LIKE '%.postgresql.org' LIMIT 10;


      ip       |         reverse
---------------+--------------------------
 63.246.23.146 | lore.postgresql.org
 66.98.251.16  | svr5.postgresql.org
 66.98.251.159 | svr4.postgresql.org
 69.168.55.12  | buildfarm.postgresql.org
 87.238.57.226 | castal.postgresql.org
 87.238.57.227 | feris.postgresql.org
 87.238.57.228 | borka.postgresql.org
 87.238.57.229 | magus.postgresql.org
 87.238.57.230 | straleb.postgresql.org
 87.238.57.231 | meldrar.postgresql.org
(10 rows)

```]

???
Not using btree index, store reversed or use index over reversed value.

---
class: center

# Scenario #2

.mcode[
```
# SELECT ip, name FROM rev WHERE name = reverse('postgresql.org')
  OR name LIKE reverse('%.postgresql.org') LIMIT 10;


      ip       |           name
---------------+--------------------------
 63.246.23.146 | gro.lqsergtsop.erol
 66.98.251.16  | gro.lqsergtsop.5rvs
 66.98.251.159 | gro.lqsergtsop.4rvs
 69.168.55.12  | gro.lqsergtsop.mrafdliub
 87.238.57.226 | gro.lqsergtsop.latsac
 87.238.57.227 | gro.lqsergtsop.siref
 87.238.57.228 | gro.lqsergtsop.akrob
 87.238.57.229 | gro.lqsergtsop.sugam
 87.238.57.230 | gro.lqsergtsop.belarts
 87.238.57.231 | gro.lqsergtsop.rardlem
(10 rows)


```]

???

---
class: center

# Scenario #2

.mcode[
```
# SELECT ip, reverse(name) FROM rev WHERE name = reverse('postgresql.org')
  OR name LIKE reverse('%.postgresql.org') LIMIT 10;


      ip       |         reverse
---------------+--------------------------
 63.246.23.146 | lore.postgresql.org
 66.98.251.16  | svr5.postgresql.org
 66.98.251.159 | svr4.postgresql.org
 69.168.55.12  | buildfarm.postgresql.org
 87.238.57.226 | castal.postgresql.org
 87.238.57.227 | feris.postgresql.org
 87.238.57.228 | borka.postgresql.org
 87.238.57.229 | magus.postgresql.org
 87.238.57.230 | straleb.postgresql.org
 87.238.57.231 | meldrar.postgresql.org
(10 rows)


```]

???
---
class: center

# The Table

.mcode[
```
CREATE TABLE rev (
    ip INET PRIMARY KEY,
    name TEXT
);

CREATE INDEX rev_name_idx
    ON rev (name);
```]

---

class: center

# The Table

.mcode[
```
      Table "public.rev"


   Column   | Type | Modifiers
------------+------+-----------
 ip         | inet | not null
 name       | text |


Indexes:
    "rev_pkey" PRIMARY KEY, btree (ip)
    "rev_name_idx" btree (name)
```
]

???
Two main scenarios
<< inet
subdomains
store reversed strings


Goal: keep everything as simple as possible, no sharding.

---
class: center

# CLUSTER

.left[
> CLUSTER instructs PostgreSQL to cluster the table specified by **table_name** based on the index specified by **index_name**. When a table is clustered, it is physically reordered based on the index information.
]

.mcode[
```
# CLUSTER rev USING rev_pkey;
```
]

???
For selection by networks it's better to have close IPs closer to each other, so there will be less page fetches from disk.

One-time operation, actually rewrites the table.

What happens during update? PTRs a more or less "static", but they actually change and it's better to run cluster again (ACCESS EXCLUSIVE lock → stop updates, create a copy of the table, cluster it, swap tables) from time to time or to specify fillfactor.

TODO: measure
---

class: center

# The Table

.mcode[
```
CREATE TABLE rev (
    ip INET PRIMARY KEY,
    name TEXT
) WITH (
    fillfactor=70
);

CREATE INDEX rev_name_idx
    ON rev (name);
```]


---
class: center

# Documentation

> The storage requirement for a short string (up to 126 bytes) is 1 byte plus the actual string, which includes the space padding in the case of character. Longer strings have 4 bytes of overhead instead of 1.

???

I read this and decided to split table to two different tables — to store long values separately.
---
class: center

# Documentation

> The storage requirement for a short string (up to 126 bytes) is 1 byte plus the actual string, which includes the space padding in the case of character. Longer strings have 4 bytes of overhead instead of 1.

# pg_column_size()

.mcode[
```
       column        | pg_column_size
---------------------+----------------
 000...000exactly120 |            121
 000...000exactly170 |            174
(2 rows)
```
]

???

12 minutes

---

# varchar_pattern_ops

Index sizes
Operation time


???
C collation, UTF8

---

class: center

# The Table

.mcode[
```
CREATE TABLE rev (
    ip INET PRIMARY KEY,
    name TEXT
) WITH (
    fillfactor=70
);

CREATE INDEX rev_name_idx
    ON rev (name varchar_pattern_ops);
```]

---
class: center

# Tracking updates

.mcode[
```
ALTER TABLE rev ADD COLUMN updated_at DATE;
```
]

<br/>

.mtable[
| ip | name | updated_at |
|:------|:---------|:------|
| fixed | variable | fixed |
]

???
It's better to store all fixed values first, but why?

Experiment with mentioned table structure.

---
class: center

# Type alignment

<br/>

.mtable[
| ip | name | updated_at |
|:------|:---------|:------|
| fixed | variable | fixed |
]

<br/>

.mtable[
| ip | updated_at | name |
|:------|:---------|:------|
| fixed | fixed | variable |
]


---
class: center

# Inserts

<br/>

.mtable[
| ip | name | updated_at |
|:------|:---------|:------|
| fixed | variable | fixed |
]

<br/>

.mtable[
| ip | updated_at | name |
|:------|:---------|:------|
| fixed | fixed | variable |
]

# No winner


---
class: center

# Seq scan by ip

<br/>

.mtable[
| ip | name | updated_at |
|:------|:---------|:------|
| **fixed** | variable | fixed |
]

<br/>

.mtable[
| ip | updated_at | name |
|:------|:---------|:------|
| **fixed** | fixed | variable |
]

# No winner
Results within measurement error.

---
class: center

# Seq scan by updated_at

<br/>

.mtable[
| ip | name | updated_at |
|:------|:---------|:------|
| fixed | variable | **fixed** |
]

<br/>

.mtable[
| ip | updated_at | name |
|:------|:---------|:------|
| fixed | **fixed** | variable |
]

### Second option is about 9% faster


---

class: center

# The Table

.mcode[
```
CREATE TABLE rev (
    ip INET PRIMARY KEY,
    updated_at DATE,
    name TEXT
) WITH (
    fillfactor=70
);

CREATE INDEX rev_name_idx
    ON rev (name varchar_pattern_ops);
```]

---

class: center

# Updates

More than one python scripts that collect data, use of intermediate tables.

???
Async scripts - it's better to write everything in buffer fast at once to not loose incoming UDP packets.

I can afford losing some update chunks.

---
class: center

# Write amplification

???
TEMPORARY (not for sharing data between processes)
UNLOGGED
LOGGED (regular)


---
class: center

# Write amplification

.mcode[```
# CREATE UNLOGGED TABLE test_inserts (f1 INT);
# INSERT INTO test_inserts SELECT generate_series(1, 10000000);
```]

### ~359MB

???
It's not the best test for Postgres because of additional 24 bytes per row, but it shows trend.

cat /proc/{}/io | grep ^write_bytes

---
class: center

# Write amplification

.mcode[```
# CREATE UNLOGGED TABLE test_inserts (f1 INT PRIMARY KEY);
# INSERT INTO test_inserts SELECT generate_series(1, 10000000);
```]

### ~560MB

---
class: center

# Write amplification

.mcode[```
# CREATE TABLE test_inserts (f1 INT);
# INSERT INTO test_inserts SELECT generate_series(1, 10000000);
```]

### ~1090MB

---
class: center

# Write amplification

.mcode[```
# CREATE TABLE test_inserts (f1 INT PRIMARY KEY);
# INSERT INTO test_inserts SELECT generate_series(1, 10000000);
```]

### ~2054MB

---

# Database

9.5/9.6

TODO: postgresql.conf settings

---

# Benchmarks

postgres@nvmepg96:~$ pgbench -i -n -q -s 100
NOTICE:  table "pgbench_history" does not exist, skipping
NOTICE:  table "pgbench_tellers" does not exist, skipping
NOTICE:  table "pgbench_accounts" does not exist, skipping
NOTICE:  table "pgbench_branches" does not exist, skipping
creating tables...
9676700 of 10000000 tuples (96%) done (elapsed 5.34 s, remaining 0.18 s)
10000000 of 10000000 tuples (100%) done (elapsed 5.50 s, remaining 0.00 s)
set primary keys...
done.

postgres@nvmepg96:~$ pgbench -t 1000 -c 1
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 100
query mode: simple
number of clients: 1
number of threads: 1
number of transactions per client: 1000
number of transactions actually processed: 1000/1000
latency average = 4.753 ms
tps = 210.376085 (including connections establishing)
tps = 210.439393 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -t 1000 -c 10
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 100
query mode: simple
number of clients: 10
number of threads: 1
number of transactions per client: 1000
number of transactions actually processed: 10000/10000
latency average = 11.087 ms
tps = 901.943743 (including connections establishing)
tps = 902.049177 (excluding connections establishing)

postgres@nvmepg96:~$ pgbench -t 1000 -c 100
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 100
query mode: simple
number of clients: 100
number of threads: 1
number of transactions per client: 1000
number of transactions actually processed: 100000/100000
latency average = 27.173 ms
tps = 3680.132455 (including connections establishing)
tps = 3680.307017 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -i -n -q -s 1000
creating tables...
9088300 of 100000000 tuples (9%) done (elapsed 5.00 s, remaining 50.02 s)
9088400 of 100000000 tuples (9%) done (elapsed 5.00 s, remaining 50.02 s)
18580600 of 100000000 tuples (18%) done (elapsed 10.00 s, remaining 43.82 s)
27544600 of 100000000 tuples (27%) done (elapsed 15.00 s, remaining 39.47 s)
37316100 of 100000000 tuples (37%) done (elapsed 20.00 s, remaining 33.60 s)
46566000 of 100000000 tuples (46%) done (elapsed 25.00 s, remaining 28.69 s)
55498800 of 100000000 tuples (55%) done (elapsed 30.00 s, remaining 24.06 s)
64744600 of 100000000 tuples (64%) done (elapsed 35.00 s, remaining 19.06 s)
72365700 of 100000000 tuples (72%) done (elapsed 40.00 s, remaining 15.27 s)
81995600 of 100000000 tuples (81%) done (elapsed 45.00 s, remaining 9.88 s)
91040000 of 100000000 tuples (91%) done (elapsed 50.00 s, remaining 4.92 s)
100000000 of 100000000 tuples (100%) done (elapsed 54.86 s, remaining 0.00 s)
set primary keys...
done.


postgres@nvmepg96:~$ pgbench -t 10000 -c 1
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 1000
query mode: simple
number of clients: 1
number of threads: 1
number of transactions per client: 10000
number of transactions actually processed: 10000/10000
latency average = 4.443 ms
tps = 225.084664 (including connections establishing)
tps = 225.092112 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -t 10000 -c 10
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 1000
query mode: simple
number of clients: 10
number of threads: 1
number of transactions per client: 1000
number of transactions actually processed: 10000/10000
latency average = 10.656 ms
tps = 938.465388 (including connections establishing)
tps = 938.577067 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -t 10000 -c 100
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 1000
query mode: simple
number of clients: 100
number of threads: 1
number of transactions per client: 1000
number of transactions actually processed: 100000/100000
latency average = 27.918 ms
tps = 3581.914626 (including connections establishing)
tps = 3582.082283 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -i -n -q -s 10000
creating tables...
9858700 of 1000000000 tuples (0%) done (elapsed 5.00 s, remaining 502.32 s)
1000000000 of 1000000000 tuples (100%) done (elapsed 545.21 s, remaining 0.00 s)
set primary keys...
done.
postgres@nvmepg96:~$ pgbench -t 10000 -c 1
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 10000
query mode: simple
number of clients: 1
number of threads: 1
number of transactions per client: 10000
number of transactions actually processed: 10000/10000
latency average = 5.001 ms
tps = 199.963435 (including connections establishing)
tps = 199.969233 (excluding connections establishing)

postgres@nvmepg96:~$ pgbench -t 10000 -c 10
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 10000
query mode: simple
number of clients: 10
number of threads: 1
number of transactions per client: 10000
number of transactions actually processed: 100000/100000
latency average = 14.264 ms
tps = 701.056576 (including connections establishing)
tps = 701.062810 (excluding connections establishing)


postgres@nvmepg96:~$ pgbench -t 10000 -c 100
starting vacuum...end.
transaction type: <builtin: TPC-B (sort of)>
scaling factor: 10000
query mode: simple
number of clients: 100
number of threads: 1
number of transactions per client: 10000
number of transactions actually processed: 1000000/1000000
latency average = 38.912 ms
tps = 2569.894441 (including connections establishing)
tps = 2569.904046 (excluding connections establishing)


---

# Data collection

python script
home router (30kpps)
DDoS detector (5kpps)

---

# Database schema

---

# Updates
Sequencer
Consistency
Unlogged tables
Update rate

---

# Data types

PostgreSQL documentation states:
> The storage requirement for a short string (up to 126 bytes) is 1 byte plus the actual string, which includes the space padding in the case of character. Longer strings have 4 bytes of overhead instead of 1.

           column        | pg_column_size
    ---------------------+----------------
     000...000exactly120 |            121
     000...000exactly170 |            174
    (2 rows)

???
I read this and decided to make a whole field VARCHAR(126) to avoid additional disk space usage, but I made a test and found that it's applicable per row, not per field.

---

# Reads
varchar_ops

---

# Search
trigram

---

# Pain
## Disc space
inet vs int
varchar(126)

test=# create table vc_120 (t varchar(120));
CREATE TABLE
Time: 6.113 ms
test=# insert into vc_120 select lpad(t::text, 120, '0') from (select generate_series(1, 50000000)) as t;
INSERT 0 50000000
Time: 124243.013 ms
test=# select pg_relation_size('vc_120'), pg_relation_size('vc_120')/50000000;
 pg_relation_size | ?column?
------------------+----------
       7876927488 |      157
(1 row)

Time: 0.880 ms

test=# create table vc_170 (t varchar(170));
CREATE TABLE
Time: 8.747 ms
test=# insert into vc_170 select lpad(t::text, 170, '0') from (select generate_series(1, 50000000)) as t;
INSERT 0 50000000
Time: 163271.730 ms
test=# select pg_relation_size('vc_170'), pg_relation_size('vc_170')/50000000;
 pg_relation_size | ?column?
------------------+----------
      10240000000 |      204
(1 row)

Time: 0.286 ms

test=# create table vc_170_120 (t varchar(170));
CREATE TABLE
Time: 5.165 ms
test=# insert into vc_170_120 select lpad(t::text, 120, '0') from (select generate_series(1, 50000000)) as t;
INSERT 0 50000000
Time: 142305.777 ms
test=# select pg_relation_size('vc_170_120'), pg_relation_size('vc_170_120')/50000000;
 pg_relation_size | ?column?
------------------+----------
       7876927488 |      157
(1 row)

Time: 0.363 ms



test=# create table vc_120_plain (t varchar(120));
CREATE TABLE
Time: 7.591 ms
test=# alter table vc_120_plain alter column t set storage plain;
ALTER TABLE
Time: 2.439 ms
test=# insert into vc_120_plain select lpad(t::text, 120, '0') from (select generate_series(1, 50000000)) as t;
INSERT 0 50000000
Time: 154441.616 ms
test=# select pg_relation_size('vc_120_plain'), pg_relation_size('vc_120_plain')/50000000;
 pg_relation_size | ?column?
------------------+----------
       7876927488 |      157
(1 row)

Time: 0.221 ms
test=# \d+ vc_120_plain
                            Table "public.vc_120_plain"
 Column |          Type          | Modifiers | Storage | Stats target | Description
--------+------------------------+-----------+---------+--------------+-------------
 t      | character varying(120) |           | plain   |              |




---

# Pain
### PG 9.5

    reverse=# vacuum analyze rev;
    VACUUM
    Time: 13063518.770 ms

### PG 9.6

    reverse=# vacuum analyze rev;
    VACUUM
    Time: 13156379.585 ms

???
No difference between versions, but no one expected
---

# Pain
~$ time pg_dump -Fc --verbose --no-unlogged-table-data reverse > ./reverse.dump.2016-10-16.fc.sql

real    24m58.552s
user    23m50.708s
sys     0m14.676s

~$ time pg_restore -d reverse /tmp/reverse.dump.2016-10-02.fc.sql

real    298m48.499s
user    6m12.956s
sys     0m36.552s
---

# Updates
TODO: do not update values that not changed
TODO: description of rows version, indexing and WAL
---

# Reads
TODO: sort in query
TODO: recursive queries

---

# Nice features

    </textarea>
    <script src="assets/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
